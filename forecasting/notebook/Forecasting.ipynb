{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from sklearn.cross_validation import  train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# to not display the warnings of tensorflow\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters to be set (\"optimum\" hyperparameters obtained from grid search):\n",
    "look_back = 4\n",
    "epochs = 2000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [40, 50, 80, 100, 84, 98, 130, 125,\n",
    "                    140, 180, 202, 219, 231, 207, 230,\n",
    "                    250, 277, 291, 302, 315, 300,\n",
    "                    312, 348, 367, 380, 350, 261, 289,\n",
    "                    320, 390, 430, 467, 419, 422, 489,\n",
    "                    512, 560, 620, 650, 710, 730, 722, 780,\n",
    "                    820, 892, 878, 902, 950, 961, 988, 946]\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i] = [dataset[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = create_dataset(train, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0464135 , 0.06118143, 0.09493671, 0.08966245, 0.10548523,\n",
       "       0.14767932, 0.17088608, 0.18881857, 0.20147679, 0.17616034,\n",
       "       0.20042194, 0.22151899, 0.25      , 0.26476793, 0.27637131,\n",
       "       0.29008439, 0.2742616 , 0.28691983, 0.32489451, 0.34493671,\n",
       "       0.35864979, 0.32700422, 0.23312236, 0.26265823, 0.29535865,\n",
       "       0.36919831, 0.41139241, 0.45042194, 0.39978903, 0.40295359,\n",
       "       0.47362869, 0.4978903 , 0.54852321, 0.61181435, 0.64345992,\n",
       "       0.70675105, 0.7278481 , 0.71940928, 0.78059072, 0.82278481,\n",
       "       0.89873418, 0.88396624, 0.9092827 , 0.95991561, 0.97151899,\n",
       "       1.        ])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input of the LSTM to be format [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\costi\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 0.2385\n",
      "Epoch 2/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.2281\n",
      "Epoch 3/200\n",
      "46/46 [==============================] - 0s 282us/step - loss: 0.2182\n",
      "Epoch 4/200\n",
      "46/46 [==============================] - 0s 282us/step - loss: 0.2094\n",
      "Epoch 5/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.1997\n",
      "Epoch 6/200\n",
      "46/46 [==============================] - 0s 326us/step - loss: 0.1915\n",
      "Epoch 7/200\n",
      "46/46 [==============================] - 0s 282us/step - loss: 0.1829\n",
      "Epoch 8/200\n",
      "46/46 [==============================] - 0s 282us/step - loss: 0.1750\n",
      "Epoch 9/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.1673\n",
      "Epoch 10/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.1591\n",
      "Epoch 11/200\n",
      "46/46 [==============================] - 0s 282us/step - loss: 0.1520\n",
      "Epoch 12/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.1454\n",
      "Epoch 13/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.1386\n",
      "Epoch 14/200\n",
      "46/46 [==============================] - 0s 217us/step - loss: 0.1317\n",
      "Epoch 15/200\n",
      "46/46 [==============================] - 0s 348us/step - loss: 0.1254\n",
      "Epoch 16/200\n",
      "46/46 [==============================] - 0s 348us/step - loss: 0.1196\n",
      "Epoch 17/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.1133\n",
      "Epoch 18/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.1078\n",
      "Epoch 19/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.1023\n",
      "Epoch 20/200\n",
      "46/46 [==============================] - 0s 391us/step - loss: 0.0968\n",
      "Epoch 21/200\n",
      "46/46 [==============================] - 0s 369us/step - loss: 0.0919\n",
      "Epoch 22/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.0871\n",
      "Epoch 23/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.0821\n",
      "Epoch 24/200\n",
      "46/46 [==============================] - 0s 348us/step - loss: 0.0775\n",
      "Epoch 25/200\n",
      "46/46 [==============================] - 0s 326us/step - loss: 0.0733\n",
      "Epoch 26/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0695\n",
      "Epoch 27/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.081 - 0s 369us/step - loss: 0.0657\n",
      "Epoch 28/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.0618\n",
      "Epoch 29/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.0585\n",
      "Epoch 30/200\n",
      "46/46 [==============================] - 0s 326us/step - loss: 0.0552\n",
      "Epoch 31/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.0521\n",
      "Epoch 32/200\n",
      "46/46 [==============================] - 0s 369us/step - loss: 0.0495\n",
      "Epoch 33/200\n",
      "46/46 [==============================] - 0s 326us/step - loss: 0.0467\n",
      "Epoch 34/200\n",
      "46/46 [==============================] - 0s 369us/step - loss: 0.0444\n",
      "Epoch 35/200\n",
      "46/46 [==============================] - 0s 326us/step - loss: 0.0423\n",
      "Epoch 36/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0402\n",
      "Epoch 37/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.0381\n",
      "Epoch 38/200\n",
      "46/46 [==============================] - 0s 369us/step - loss: 0.0368\n",
      "Epoch 39/200\n",
      "46/46 [==============================] - 0s 326us/step - loss: 0.0353\n",
      "Epoch 40/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.0341\n",
      "Epoch 41/200\n",
      "46/46 [==============================] - 0s 348us/step - loss: 0.0328\n",
      "Epoch 42/200\n",
      "46/46 [==============================] - 0s 348us/step - loss: 0.0319\n",
      "Epoch 43/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0311\n",
      "Epoch 44/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.0305\n",
      "Epoch 45/200\n",
      "46/46 [==============================] - 0s 282us/step - loss: 0.0296\n",
      "Epoch 46/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.0292\n",
      "Epoch 47/200\n",
      "46/46 [==============================] - 0s 326us/step - loss: 0.0288\n",
      "Epoch 48/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0283\n",
      "Epoch 49/200\n",
      "46/46 [==============================] - 0s 348us/step - loss: 0.0278\n",
      "Epoch 50/200\n",
      "46/46 [==============================] - 0s 326us/step - loss: 0.0274\n",
      "Epoch 51/200\n",
      "46/46 [==============================] - 0s 326us/step - loss: 0.0270\n",
      "Epoch 52/200\n",
      "46/46 [==============================] - 0s 326us/step - loss: 0.0265\n",
      "Epoch 53/200\n",
      "46/46 [==============================] - 0s 282us/step - loss: 0.0262\n",
      "Epoch 54/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0258\n",
      "Epoch 55/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.0254\n",
      "Epoch 56/200\n",
      "46/46 [==============================] - 0s 217us/step - loss: 0.0250\n",
      "Epoch 57/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0246\n",
      "Epoch 58/200\n",
      "46/46 [==============================] - 0s 326us/step - loss: 0.0242\n",
      "Epoch 59/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.023 - 0s 282us/step - loss: 0.0238\n",
      "Epoch 60/200\n",
      "46/46 [==============================] - 0s 326us/step - loss: 0.0234\n",
      "Epoch 61/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0231\n",
      "Epoch 62/200\n",
      "46/46 [==============================] - 0s 217us/step - loss: 0.0227\n",
      "Epoch 63/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.0223\n",
      "Epoch 64/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0219\n",
      "Epoch 65/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0215\n",
      "Epoch 66/200\n",
      "46/46 [==============================] - 0s 217us/step - loss: 0.0212\n",
      "Epoch 67/200\n",
      "46/46 [==============================] - 0s 217us/step - loss: 0.0208\n",
      "Epoch 68/200\n",
      "46/46 [==============================] - 0s 326us/step - loss: 0.0204\n",
      "Epoch 69/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.0201\n",
      "Epoch 70/200\n",
      "46/46 [==============================] - 0s 282us/step - loss: 0.0197\n",
      "Epoch 71/200\n",
      "46/46 [==============================] - 0s 282us/step - loss: 0.0194\n",
      "Epoch 72/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.019 - 0s 261us/step - loss: 0.0190\n",
      "Epoch 73/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.0186\n",
      "Epoch 74/200\n",
      "46/46 [==============================] - 0s 282us/step - loss: 0.0183\n",
      "Epoch 75/200\n",
      "46/46 [==============================] - 0s 326us/step - loss: 0.0179\n",
      "Epoch 76/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.0176\n",
      "Epoch 77/200\n",
      "46/46 [==============================] - 0s 282us/step - loss: 0.0172\n",
      "Epoch 78/200\n",
      "46/46 [==============================] - 0s 217us/step - loss: 0.0169\n",
      "Epoch 79/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.0165\n",
      "Epoch 80/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.0162\n",
      "Epoch 81/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.0158\n",
      "Epoch 82/200\n",
      "46/46 [==============================] - 0s 282us/step - loss: 0.0155\n",
      "Epoch 83/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.0152\n",
      "Epoch 84/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0149\n",
      "Epoch 85/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.0145\n",
      "Epoch 86/200\n",
      "46/46 [==============================] - 0s 217us/step - loss: 0.0142\n",
      "Epoch 87/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.0139\n",
      "Epoch 88/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0136\n",
      "Epoch 89/200\n",
      "46/46 [==============================] - 0s 217us/step - loss: 0.0133\n",
      "Epoch 90/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.0130\n",
      "Epoch 91/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.0127\n",
      "Epoch 92/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0124\n",
      "Epoch 93/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.0121\n",
      "Epoch 94/200\n",
      "46/46 [==============================] - 0s 282us/step - loss: 0.0118\n",
      "Epoch 95/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.0115\n",
      "Epoch 96/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.0112\n",
      "Epoch 97/200\n",
      "46/46 [==============================] - 0s 217us/step - loss: 0.0109\n",
      "Epoch 98/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.0106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/200\n",
      "46/46 [==============================] - 0s 326us/step - loss: 0.0103\n",
      "Epoch 100/200\n",
      "46/46 [==============================] - 0s 217us/step - loss: 0.0101\n",
      "Epoch 101/200\n",
      "46/46 [==============================] - 0s 217us/step - loss: 0.0098\n",
      "Epoch 102/200\n",
      "46/46 [==============================] - 0s 217us/step - loss: 0.0096\n",
      "Epoch 103/200\n",
      "46/46 [==============================] - 0s 217us/step - loss: 0.0093\n",
      "Epoch 104/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0090\n",
      "Epoch 105/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0088\n",
      "Epoch 106/200\n",
      "46/46 [==============================] - 0s 217us/step - loss: 0.0085\n",
      "Epoch 107/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.0083\n",
      "Epoch 108/200\n",
      "46/46 [==============================] - 0s 348us/step - loss: 0.0081\n",
      "Epoch 109/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.0079\n",
      "Epoch 110/200\n",
      "46/46 [==============================] - 0s 348us/step - loss: 0.0076\n",
      "Epoch 111/200\n",
      "46/46 [==============================] - 0s 282us/step - loss: 0.0074\n",
      "Epoch 112/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.0072\n",
      "Epoch 113/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0070\n",
      "Epoch 114/200\n",
      "46/46 [==============================] - 0s 282us/step - loss: 0.0068\n",
      "Epoch 115/200\n",
      "46/46 [==============================] - 0s 282us/step - loss: 0.0066\n",
      "Epoch 116/200\n",
      "46/46 [==============================] - 0s 348us/step - loss: 0.0064\n",
      "Epoch 117/200\n",
      "46/46 [==============================] - 0s 282us/step - loss: 0.0062\n",
      "Epoch 118/200\n",
      "46/46 [==============================] - 0s 282us/step - loss: 0.0060\n",
      "Epoch 119/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.0058\n",
      "Epoch 120/200\n",
      "46/46 [==============================] - 0s 326us/step - loss: 0.0057\n",
      "Epoch 121/200\n",
      "46/46 [==============================] - 0s 348us/step - loss: 0.0055\n",
      "Epoch 122/200\n",
      "46/46 [==============================] - 0s 326us/step - loss: 0.0054\n",
      "Epoch 123/200\n",
      "46/46 [==============================] - 0s 369us/step - loss: 0.0052\n",
      "Epoch 124/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.0051\n",
      "Epoch 125/200\n",
      "46/46 [==============================] - 0s 348us/step - loss: 0.0049\n",
      "Epoch 126/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0048\n",
      "Epoch 127/200\n",
      "46/46 [==============================] - 0s 217us/step - loss: 0.0047\n",
      "Epoch 128/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.0045\n",
      "Epoch 129/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.0044\n",
      "Epoch 130/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0043\n",
      "Epoch 131/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.0042\n",
      "Epoch 132/200\n",
      "46/46 [==============================] - 0s 217us/step - loss: 0.0041\n",
      "Epoch 133/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.0040\n",
      "Epoch 134/200\n",
      "46/46 [==============================] - 0s 348us/step - loss: 0.0039\n",
      "Epoch 135/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0038\n",
      "Epoch 136/200\n",
      "46/46 [==============================] - 0s 326us/step - loss: 0.0037\n",
      "Epoch 137/200\n",
      "46/46 [==============================] - 0s 282us/step - loss: 0.0036\n",
      "Epoch 138/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0035\n",
      "Epoch 139/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.0034\n",
      "Epoch 140/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.0033\n",
      "Epoch 141/200\n",
      "46/46 [==============================] - 0s 217us/step - loss: 0.0033\n",
      "Epoch 142/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0032\n",
      "Epoch 143/200\n",
      "46/46 [==============================] - 0s 282us/step - loss: 0.0031\n",
      "Epoch 144/200\n",
      "46/46 [==============================] - 0s 326us/step - loss: 0.0031\n",
      "Epoch 145/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0030\n",
      "Epoch 146/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.0030\n",
      "Epoch 147/200\n",
      "46/46 [==============================] - 0s 326us/step - loss: 0.0029\n",
      "Epoch 148/200\n",
      "46/46 [==============================] - 0s 369us/step - loss: 0.0029\n",
      "Epoch 149/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.0028\n",
      "Epoch 150/200\n",
      "46/46 [==============================] - 0s 348us/step - loss: 0.0028\n",
      "Epoch 151/200\n",
      "46/46 [==============================] - 0s 282us/step - loss: 0.0027\n",
      "Epoch 152/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0027\n",
      "Epoch 153/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.0026\n",
      "Epoch 154/200\n",
      "46/46 [==============================] - 0s 369us/step - loss: 0.0026\n",
      "Epoch 155/200\n",
      "46/46 [==============================] - 0s 217us/step - loss: 0.0026\n",
      "Epoch 156/200\n",
      "46/46 [==============================] - 0s 282us/step - loss: 0.0025\n",
      "Epoch 157/200\n",
      "46/46 [==============================] - 0s 348us/step - loss: 0.0025\n",
      "Epoch 158/200\n",
      "46/46 [==============================] - 0s 217us/step - loss: 0.0025\n",
      "Epoch 159/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.0025\n",
      "Epoch 160/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.0024\n",
      "Epoch 161/200\n",
      "46/46 [==============================] - 0s 391us/step - loss: 0.0024\n",
      "Epoch 162/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.0024\n",
      "Epoch 163/200\n",
      "46/46 [==============================] - 0s 326us/step - loss: 0.0024\n",
      "Epoch 164/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.0023\n",
      "Epoch 165/200\n",
      "46/46 [==============================] - 0s 370us/step - loss: 0.0023\n",
      "Epoch 166/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.0023\n",
      "Epoch 167/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.0023\n",
      "Epoch 168/200\n",
      "46/46 [==============================] - 0s 326us/step - loss: 0.0023\n",
      "Epoch 169/200\n",
      "46/46 [==============================] - 0s 282us/step - loss: 0.0023\n",
      "Epoch 170/200\n",
      "46/46 [==============================] - 0s 282us/step - loss: 0.0022\n",
      "Epoch 171/200\n",
      "46/46 [==============================] - 0s 282us/step - loss: 0.0022\n",
      "Epoch 172/200\n",
      "46/46 [==============================] - 0s 282us/step - loss: 0.0022\n",
      "Epoch 173/200\n",
      "46/46 [==============================] - 0s 196us/step - loss: 0.0022\n",
      "Epoch 174/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.0022\n",
      "Epoch 175/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0022\n",
      "Epoch 176/200\n",
      "46/46 [==============================] - 0s 326us/step - loss: 0.0022\n",
      "Epoch 177/200\n",
      "46/46 [==============================] - 0s 195us/step - loss: 0.0022\n",
      "Epoch 178/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.0022\n",
      "Epoch 179/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.002 - 0s 326us/step - loss: 0.0022\n",
      "Epoch 180/200\n",
      "46/46 [==============================] - 0s 282us/step - loss: 0.0022\n",
      "Epoch 181/200\n",
      "46/46 [==============================] - 0s 239us/step - loss: 0.0022\n",
      "Epoch 182/200\n",
      "46/46 [==============================] - 0s 326us/step - loss: 0.0022\n",
      "Epoch 183/200\n",
      "46/46 [==============================] - 0s 282us/step - loss: 0.0022\n",
      "Epoch 184/200\n",
      "46/46 [==============================] - 0s 196us/step - loss: 0.0022\n",
      "Epoch 185/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0021\n",
      "Epoch 186/200\n",
      "46/46 [==============================] - 0s 326us/step - loss: 0.0021\n",
      "Epoch 187/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0021\n",
      "Epoch 188/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0021\n",
      "Epoch 189/200\n",
      "46/46 [==============================] - 0s 348us/step - loss: 0.0021\n",
      "Epoch 190/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.002 - 0s 239us/step - loss: 0.0021\n",
      "Epoch 191/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0021\n",
      "Epoch 192/200\n",
      "46/46 [==============================] - 0s 326us/step - loss: 0.0021\n",
      "Epoch 193/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0021\n",
      "Epoch 194/200\n",
      "46/46 [==============================] - 0s 217us/step - loss: 0.0021\n",
      "Epoch 195/200\n",
      "46/46 [==============================] - 0s 326us/step - loss: 0.0021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0021\n",
      "Epoch 197/200\n",
      "46/46 [==============================] - 0s 217us/step - loss: 0.0021\n",
      "Epoch 198/200\n",
      "46/46 [==============================] - 0s 304us/step - loss: 0.0021\n",
      "Epoch 199/200\n",
      "46/46 [==============================] - 0s 326us/step - loss: 0.0021\n",
      "Epoch 200/200\n",
      "46/46 [==============================] - 0s 261us/step - loss: 0.0021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c17a13ff28>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(8, input_shape=(look_back, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(trainX, trainY, nb_epoch=200, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "trainPredict = model.predict(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPredict = scaler.inverse_transform(trainPredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.243623e+08]], dtype=float32)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift predictions of training data for plotting\n",
    "trainPredictPlot = np.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c17bfb2d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.01054852],\n",
       "       [0.04219409],\n",
       "       [0.06329114]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.8839662447257383],\n",
       " [0.9092827004219408],\n",
       " [0.9599156118143458],\n",
       " [0.9715189873417721]]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = trainX[len(trainX) - 1]\n",
    "s = [list(x) for x in l]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.88396624],\n",
       "        [0.9092827 ],\n",
       "        [0.95991561],\n",
       "        [0.97151899]]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([s])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[924362300.0]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = scaler.inverse_transform(pred)\n",
    "list(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.9092827004219408],\n",
       " [0.9599156118143458],\n",
       " [0.9715189873417721],\n",
       " [924362300.0]]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = []\n",
    "for i in range(1, len(s)):\n",
    "    k.append(s[i])\n",
    "k.append(list(pred[0]))\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1028.508]]\n",
      "[[1074.8751]]\n",
      "[[1127.3496]]\n",
      "[[1181.7805]]\n",
      "[[1248.9644]]\n",
      "[[1316.2633]]\n",
      "[[1389.5437]]\n",
      "[[1468.7815]]\n",
      "[[1554.8303]]\n",
      "[[1645.6648]]\n"
     ]
    }
   ],
   "source": [
    "l = trainX[len(trainX) - 1]\n",
    "s = [list(x) for x in l]\n",
    "for it in range(10):\n",
    "    t = np.array([s])\n",
    "    pred = model.predict(t)\n",
    "    predt = scaler.inverse_transform(pred)\n",
    "    k = []\n",
    "    for i in range(1, len(s)):\n",
    "        k.append(s[i])\n",
    "    k.append(list(pred[0]))\n",
    "    s = k\n",
    "    print (predt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
